{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\" markdown=\"1\">Encoder Layer</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask =None): # To calculate self-attention\n",
    "    # q,k,v = 30 X 8 X 200 X64\n",
    "    d_k =q.size()[-1] # 64\n",
    "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k) # 30 X 8 X 200 X 200\n",
    "    print(f\"scaled.size(): {scaled.size()} --\")\n",
    "    if mask is not None:\n",
    "        print(f\" -- Adding MMMask of shape {mask.size()} --\")\n",
    "        scaled += mask # 30 X 8 X 200 X 200\n",
    "    attention = F.softmax(scaled, dim= -1) # 30 X 8 X 200 X 200\n",
    "    values = torch.matmul(attention, v) # 30 X 8 X 200 X 64 // 64 is here the embedding of the value tensor\n",
    "    return values, attention # this new value vector is much more context aware now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # 512\n",
    "        self.num_heads =num_heads # 8\n",
    "        self.head_dim = d_model//num_heads # 64\n",
    "        self.qkv_layer = nn.Linear(d_model, 3*d_model) # 512 X 1536\n",
    "        self.linear_layer = nn.Linear(d_model, d_model) # 512 X 512\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, sequence_length, d_model = x.size() # 30 X 200 X 512\n",
    "        print(f\"x.size(): {x.size()}\")\n",
    "\n",
    "        qkv = self.qkv_layer(x) # 30 X 200 X 1536\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "\n",
    "        qkv = qkv.reshape( batch_size, sequence_length, self.num_heads, 3*self.head_dim) # 30 X 200 X 8 X 192\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "\n",
    "        qkv = qkv.permute( 0, 2, 1, 3) # 30 X 8 X 200 X 192\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "\n",
    "        q, k, v = qkv.chunk(3, dim=-1) # each are 30 X 8 X 200 X 64 // we will divide each chunck for each head\n",
    "        print(f\"q size: {q.size()}, k size: {k.size()}, v size: {v.size()} \")\n",
    "\n",
    "        values, attention = scaled_dot_product(q, k, v, mask) # attention = 30 X 8 X 200 X 200 || Values = 30 X 8 X 200 X 64\n",
    "        print(f\"values.size(): {values.size()} , attention.size() : {attention.size()}\")\n",
    "\n",
    "        values = values.reshape( batch_size, sequence_length, self.num_heads*self.head_dim) # 30 X 200 X 512(8*64)\n",
    "        print(f\"values.size(): {values.size()}\")\n",
    "\n",
    "        out = self.linear_layer(values)\n",
    "        print(f\"out.size(): {out.size()}\")\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, parameters_shape, eps = 1e-5):\n",
    "        super().__init__()\n",
    "        self.parameters_shape=parameters_shape # [512] // along which dimension we want to perform layer normalization\n",
    "        self.eps=eps # small value to prevent denm. from getting zero\n",
    "        self.gamma=nn.Parameter(torch.ones(parameters_shape)) # [512] // S.D values\n",
    "        self.beta =nn.Parameter(torch.ones(parameters_shape)) # [512] // Mean values\n",
    "    \n",
    "    def forward(self,inputs): # 30 X 200 X 512\n",
    "        dims = [-(i+1) for i in range(len(self.parameters_shape))] #[-1] // the last dimension along which we want to perform layer normalization.\n",
    "        \n",
    "        mean = inputs.mean(dim=dims, keepdim=True) # 30 X 200 X 1\n",
    "        print(f\" Mean size: {mean.size()}\")\n",
    "        \n",
    "        var = ((inputs-mean)**2).mean(dim=dims, keepdim=True) # 30 X 200 X 1\n",
    "        std=(var+self.eps).sqrt() # 30 X 200 X 1\n",
    "        print(f\"S.D size: {std.size()}\")\n",
    "        \n",
    "        y=(inputs-mean)/std # 30 X 200 X 512\n",
    "        print(f\"y size: {y.size()}\")\n",
    "        \n",
    "        out=self.gamma*y + self.beta # we have 512 gammas & betas for 1 for each sequence in each batch \n",
    "        print(f\"self.gamma: {self.gamma.size()}, self.beta: {self.beta.size()}\") # Both are learnable parameters\n",
    "        print(f\"out size: {out.size()}\")\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After layer Norm we are going to get much more stable values. And now we are going to pass with set of linear layer with activation func (ReLU) and Dropout.\n",
    "\n",
    ">\n",
    "    * Linear layer is going help have btter intereaction between different head values we just concatenated.\n",
    "    * ReLU helps n/w to understand much better and complex patterns.\n",
    "    * Dropout help to make n/w genearlize in better way and not just focus on specific type of pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward,self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden) # 512 X 2048\n",
    "        self.linear2 = nn.Linear(hidden, d_model) # 2048 x 512\n",
    "\n",
    "        self.relu = nn.ReLU() # Activation function // max(0,x)\n",
    "\n",
    "        self.dropout = nn.Dropout(p = drop_prob)\n",
    "    \n",
    "    def forward(self, x): # 30 X 200 X 512\n",
    "        x = self.linear1(x) # 30 X 200 X 2048\n",
    "        print(f\"x after first linear layer: {x.size()}\")\n",
    "\n",
    "        x = self.relu(x) # 30 X 200 X 2048\n",
    "        print(f\"x after relu activation: {x.size()}\")\n",
    "\n",
    "        x = self.dropout(x) # 30 X 200 X 2048\n",
    "        print(f\"x after dropout: {x.size()}\")\n",
    "\n",
    "        x = self.linear2(x) # 30 X 200 X 512\n",
    "        print(f\"x after second linear layer: {x.size()}\")\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads) # To perform mmulti-head self attention of encoder\n",
    "        self.norm1 = LayerNormalization(parameters_shape = [d_model])\n",
    "        self.dropout1=nn.Dropout(p = drop_prob)\n",
    "\n",
    "        self.ffn = PositionwiseFeedForward(d_model = d_model, hidden = ffn_hidden, drop_prob = drop_prob)\n",
    "\n",
    "        self.norm2 = LayerNormalization(parameters_shape = [d_model])\n",
    "        self.dropout2 = nn.Dropout( p =drop_prob)\n",
    "\n",
    "    def forward(self, x): # arranged in similar order as original encoder\n",
    "\n",
    "        residual_x = x # 30 X 200 X 512 \n",
    "        print(\" ----- ATTENTION 1 ----- \")\n",
    "        x = self.attention(x, mask = None) # 30 X 200 X 512\n",
    "        print(\" ----- DROPOUT 1 ----- \")\n",
    "        x = self.dropout1(x) # 30 X 200 X 512\n",
    "        print(\" ----- ADD AND LAYER NORMALIZATION 1 ----- \")\n",
    "        x = self.norm1(x + residual_x) # 30 X 200 X 512\n",
    "        residual_x = x # 30 X 200 X 512\n",
    "\n",
    "        print(\" ----- ATTENTION 2 ----- \")\n",
    "        x = self.ffn(x) # 30 X 200 X 512\n",
    "        print(\" ----- DROPOUT 2 ----- \")\n",
    "        x = self.dropout2(x) # 30 X 200 X 512\n",
    "        print(\" ----- ADD AND LAYER NORMALIZATION 2 ----- \")\n",
    "        x = self.norm2(x + residual_x) # 30 X 200 X 512\n",
    "\n",
    "        return x # This x is now much more context aware compared to x which we recieved as input.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module): # module is kind of super class we are inheriting the forward method, device selection, saving checkpoints etc.\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(*[EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n",
    "                                      for _ in range(num_layers)])\n",
    "        # This line creates a list of EncoderLayer instances, one for each number of layers specified by num_layers. \n",
    "        # It then unpacks this list into nn.Sequential, which chains these layers together so that the output of one layer \n",
    "        # becomes the input of the next.\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512 # Size of every single vector\n",
    "num_heads = 8 # no. of head\n",
    "drop_prob = 0.1 # number of neurons will be off during forward + backward pass here it is 10%\n",
    "batch_size = 30 # no. of sequence in each batch\n",
    "max_seq_len = 200 # max. lenght of sequence allowed\n",
    "ffn_hidden = 2048 # Feed Forward layer ( to expand number of neurons from 512 to 2048 and back to 512)\n",
    "num_layers = 5 # no. of encoder layers to be repeated\n",
    "\n",
    "encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ----- ATTENTION 1 ----- \n",
      "x.size(): torch.Size([30, 200, 512])\n",
      "qkv.size(): torch.Size([30, 200, 1536])\n",
      "qkv.size(): torch.Size([30, 200, 8, 192])\n",
      "qkv.size(): torch.Size([30, 8, 200, 192])\n",
      "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]) \n",
      "scaled.size(): torch.Size([30, 8, 200, 200]) --\n",
      "values.size(): torch.Size([30, 8, 200, 64]) , attention.size() : torch.Size([30, 8, 200, 200])\n",
      "values.size(): torch.Size([30, 200, 512])\n",
      "out.size(): torch.Size([30, 200, 512])\n",
      " ----- DROPOUT 1 ----- \n",
      " ----- ADD AND LAYER NORMALIZATION 1 ----- \n",
      " Mean size: torch.Size([30, 200, 1])\n",
      "S.D size: torch.Size([30, 200, 1])\n",
      "y size: torch.Size([30, 200, 512])\n",
      "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
      "out size: torch.Size([30, 200, 512])\n",
      " ----- ATTENTION 2 ----- \n",
      "x after first linear layer: torch.Size([30, 200, 2048])\n",
      "x after relu activation: torch.Size([30, 200, 2048])\n",
      "x after dropout: torch.Size([30, 200, 2048])\n",
      "x after second linear layer: torch.Size([30, 200, 512])\n",
      " ----- DROPOUT 2 ----- \n",
      " ----- ADD AND LAYER NORMALIZATION 2 ----- \n",
      " Mean size: torch.Size([30, 200, 1])\n",
      "S.D size: torch.Size([30, 200, 1])\n",
      "y size: torch.Size([30, 200, 512])\n",
      "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
      "out size: torch.Size([30, 200, 512])\n",
      " ----- ATTENTION 1 ----- \n",
      "x.size(): torch.Size([30, 200, 512])\n",
      "qkv.size(): torch.Size([30, 200, 1536])\n",
      "qkv.size(): torch.Size([30, 200, 8, 192])\n",
      "qkv.size(): torch.Size([30, 8, 200, 192])\n",
      "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]) \n",
      "scaled.size(): torch.Size([30, 8, 200, 200]) --\n",
      "values.size(): torch.Size([30, 8, 200, 64]) , attention.size() : torch.Size([30, 8, 200, 200])\n",
      "values.size(): torch.Size([30, 200, 512])\n",
      "out.size(): torch.Size([30, 200, 512])\n",
      " ----- DROPOUT 1 ----- \n",
      " ----- ADD AND LAYER NORMALIZATION 1 ----- \n",
      " Mean size: torch.Size([30, 200, 1])\n",
      "S.D size: torch.Size([30, 200, 1])\n",
      "y size: torch.Size([30, 200, 512])\n",
      "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
      "out size: torch.Size([30, 200, 512])\n",
      " ----- ATTENTION 2 ----- \n",
      "x after first linear layer: torch.Size([30, 200, 2048])\n",
      "x after relu activation: torch.Size([30, 200, 2048])\n",
      "x after dropout: torch.Size([30, 200, 2048])\n",
      "x after second linear layer: torch.Size([30, 200, 512])\n",
      " ----- DROPOUT 2 ----- \n",
      " ----- ADD AND LAYER NORMALIZATION 2 ----- \n",
      " Mean size: torch.Size([30, 200, 1])\n",
      "S.D size: torch.Size([30, 200, 1])\n",
      "y size: torch.Size([30, 200, 512])\n",
      "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
      "out size: torch.Size([30, 200, 512])\n",
      " ----- ATTENTION 1 ----- \n",
      "x.size(): torch.Size([30, 200, 512])\n",
      "qkv.size(): torch.Size([30, 200, 1536])\n",
      "qkv.size(): torch.Size([30, 200, 8, 192])\n",
      "qkv.size(): torch.Size([30, 8, 200, 192])\n",
      "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]) \n",
      "scaled.size(): torch.Size([30, 8, 200, 200]) --\n",
      "values.size(): torch.Size([30, 8, 200, 64]) , attention.size() : torch.Size([30, 8, 200, 200])\n",
      "values.size(): torch.Size([30, 200, 512])\n",
      "out.size(): torch.Size([30, 200, 512])\n",
      " ----- DROPOUT 1 ----- \n",
      " ----- ADD AND LAYER NORMALIZATION 1 ----- \n",
      " Mean size: torch.Size([30, 200, 1])\n",
      "S.D size: torch.Size([30, 200, 1])\n",
      "y size: torch.Size([30, 200, 512])\n",
      "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
      "out size: torch.Size([30, 200, 512])\n",
      " ----- ATTENTION 2 ----- \n",
      "x after first linear layer: torch.Size([30, 200, 2048])\n",
      "x after relu activation: torch.Size([30, 200, 2048])\n",
      "x after dropout: torch.Size([30, 200, 2048])\n",
      "x after second linear layer: torch.Size([30, 200, 512])\n",
      " ----- DROPOUT 2 ----- \n",
      " ----- ADD AND LAYER NORMALIZATION 2 ----- \n",
      " Mean size: torch.Size([30, 200, 1])\n",
      "S.D size: torch.Size([30, 200, 1])\n",
      "y size: torch.Size([30, 200, 512])\n",
      "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
      "out size: torch.Size([30, 200, 512])\n",
      " ----- ATTENTION 1 ----- \n",
      "x.size(): torch.Size([30, 200, 512])\n",
      "qkv.size(): torch.Size([30, 200, 1536])\n",
      "qkv.size(): torch.Size([30, 200, 8, 192])\n",
      "qkv.size(): torch.Size([30, 8, 200, 192])\n",
      "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]) \n",
      "scaled.size(): torch.Size([30, 8, 200, 200]) --\n",
      "values.size(): torch.Size([30, 8, 200, 64]) , attention.size() : torch.Size([30, 8, 200, 200])\n",
      "values.size(): torch.Size([30, 200, 512])\n",
      "out.size(): torch.Size([30, 200, 512])\n",
      " ----- DROPOUT 1 ----- \n",
      " ----- ADD AND LAYER NORMALIZATION 1 ----- \n",
      " Mean size: torch.Size([30, 200, 1])\n",
      "S.D size: torch.Size([30, 200, 1])\n",
      "y size: torch.Size([30, 200, 512])\n",
      "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
      "out size: torch.Size([30, 200, 512])\n",
      " ----- ATTENTION 2 ----- \n",
      "x after first linear layer: torch.Size([30, 200, 2048])\n",
      "x after relu activation: torch.Size([30, 200, 2048])\n",
      "x after dropout: torch.Size([30, 200, 2048])\n",
      "x after second linear layer: torch.Size([30, 200, 512])\n",
      " ----- DROPOUT 2 ----- \n",
      " ----- ADD AND LAYER NORMALIZATION 2 ----- \n",
      " Mean size: torch.Size([30, 200, 1])\n",
      "S.D size: torch.Size([30, 200, 1])\n",
      "y size: torch.Size([30, 200, 512])\n",
      "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
      "out size: torch.Size([30, 200, 512])\n",
      " ----- ATTENTION 1 ----- \n",
      "x.size(): torch.Size([30, 200, 512])\n",
      "qkv.size(): torch.Size([30, 200, 1536])\n",
      "qkv.size(): torch.Size([30, 200, 8, 192])\n",
      "qkv.size(): torch.Size([30, 8, 200, 192])\n",
      "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]) \n",
      "scaled.size(): torch.Size([30, 8, 200, 200]) --\n",
      "values.size(): torch.Size([30, 8, 200, 64]) , attention.size() : torch.Size([30, 8, 200, 200])\n",
      "values.size(): torch.Size([30, 200, 512])\n",
      "out.size(): torch.Size([30, 200, 512])\n",
      " ----- DROPOUT 1 ----- \n",
      " ----- ADD AND LAYER NORMALIZATION 1 ----- \n",
      " Mean size: torch.Size([30, 200, 1])\n",
      "S.D size: torch.Size([30, 200, 1])\n",
      "y size: torch.Size([30, 200, 512])\n",
      "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
      "out size: torch.Size([30, 200, 512])\n",
      " ----- ATTENTION 2 ----- \n",
      "x after first linear layer: torch.Size([30, 200, 2048])\n",
      "x after relu activation: torch.Size([30, 200, 2048])\n",
      "x after dropout: torch.Size([30, 200, 2048])\n",
      "x after second linear layer: torch.Size([30, 200, 512])\n",
      " ----- DROPOUT 2 ----- \n",
      " ----- ADD AND LAYER NORMALIZATION 2 ----- \n",
      " Mean size: torch.Size([30, 200, 1])\n",
      "S.D size: torch.Size([30, 200, 1])\n",
      "y size: torch.Size([30, 200, 512])\n",
      "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
      "out size: torch.Size([30, 200, 512])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((batch_size, max_seq_len, d_model)) # includes the positional encoding\n",
    "out = encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
