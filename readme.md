# Transformer explained with Code

### In this repository, I have explained the complete architecture of Transformer with the help of code.

1. **1_self_attention:** In this code the complete mechanism of self-attention is explained.

2. **2_multi_head_attention:** In this code the multi-head mechanism is explained.

3. **3_positional_encoding:** In this code the positional embedding mechanism is explained.

4. **4_Layer_normalization:** In this code the ADD & Norm mechanism is explained.

5. **5_1_Encoder_architecture:** In this code the complete go through of encoder single layer is given. (_5_2_Encoder_architecture_ is just the .py version of same code.)

6. **Decoder_architecture:** In this code the the complete go through of Decoder single layer is given.

7. **7_sentence_tokenization:** In this code the Tokenization mechanism is explained.

8. **8_complete_transformer:** In this code I have constructed the complete transformer model. (_with help of above code_)

---

> Key things to note:\
> **Query**: What I'm looking for\
> **Key**: What I've to offer\
> **Value**: what I actually offer

## References Used

1. [Batch Layer Normalization - Pinecone](https://www.pinecone.io/learn/batch-layer-normalization/)
2. [Stack Overflow: How is the number of parameters calculated in BERT model?](https://stackoverflow.com/questions/64485777/how-is-the-number-of-parameters-be-calculated-in-bert-model)
3. [YouTube: Code Emporium](https://www.youtube.com/@CodeEmporium)
4. [Attention Is All You Need - Arxiv Paper](https://arxiv.org/abs/1706.03762)
5. [The Illustrated Transformer - Jay Alammar's Blog](https://jalammar.github.io/illustrated-transformer/)
